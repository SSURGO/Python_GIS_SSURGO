---
title: "Batch functions for downloading and processing the 10-meter NED"
author: "Stephen Roecker"
date: "`r Sys.Date()`"
output: 
  html_document: 
    number_sections: yes
    toc: yes
    toc_float:
      collapsed: yes
      smooth_scroll: no
editor_options: 
  chunk_output_type: console
---

```{r setup, echo = FALSE}
knitr::opts_chunk$set(message = FALSE, warning = FALSE, eval=FALSE)

options(stringsAsFactors = FALSE)
```

# Introduction

This document displays some R batch functions for downloading, mosaicing, warping, adding pyramids, and calculating terrain derivatives from the USGS seamless 10-meter NED (National Elevation Dataset), using the R package gdalUtils. As a bonus many of the raster outputs are tiled and compressed. This shrunk some rasters to a third of their original size, and also increased the rendering speed.

The batch commands are designed to run again the NED tile index, the NLCD dataset, and the SAPOLYGON layer for each MLRA office. Also it presumes a certain file organization structure, in order to write the files to their respective folders.

The primary workhorse of these batch functions is GDAL (Geospatial Data Abstraction Library). GDAL is a popular library for reading, writing, and converting various raster and vector formats, and is incorporated into most GIS software, including ArcGIS since version 10.0. The particuar version I used came included with QGIS (which is CCE approved).

Once you begin this sequence of commands will last several days. However it is not terribly memory intensize, so you should be able to work on other projects while it is running. The first thing that needs to be done, is loading the necessary R libraries. If they're not already installed you will have to do this the first time (e.g. "install.packages("gdalUtils", dependencies=TRUE)").

```{r install and load packages}
source("C:/workspace2/github/ncss-tech/geo-pit/R-geoprocessing/nedFunctions.R")
source("C:/workspace2/github/ncss-tech/geo-pit/R-geoprocessing/gdalUtilsFunctions.R")

library(gdalUtils)
library(rgdal)
library(raster)
library(sf)
library(FedData)

```

Next the proper GDAL path has to be set. The first location is the default path on my work computer, the second my personal computer. If this isn't set gdalUtils will do a brute force search of your computer, which usually finds GDAL 1.7 instead of the GDAL 10.1. The new version has additional features, which many these batch functions use.

```{r set gdal path}
gdal_setInstallation(search_path="C:/Program Files/QGIS 3.4/bin")
```

Next numerous parameters need to be set which get used later by many of the functions or commands. Modify these file paths and lists as necessary. For example, I organized my files by "D:/geodata/project_data/11ATL"", so 11 will have to replace by 10 or 2 for your respective mlraoffices.

```{r}
# Set parameters
mlrassoarea <- paste0("11-", c("ATL", "AUR", "MAN", "CLI", "FIN", "GAL", "IND", "JUE", "SPR", "UNI", "WAV"))
mlraoffice <- c("11")
crsarg <- "+init=epsg:5070"

# construct table of geodata
gd <- {
  rbind(
    expand.grid(variable   = c("nlcd"),
               resolution  = c("30m"),
               folder = c(mlrassoarea, mlraoffice),
               stringsAsFactors = FALSE
               ),
     expand.grid(variable  = c("ned"),
                 resolution = c("09d", "10m", "30m"),
                 folder     = c(mlrassoarea, mlraoffice),
                 stringsAsFactors = FALSE
                 )
    ) ->.;
  within(., {
    var_res   = paste0(variable, resolution)
    file_path = paste0("M:/geodata/project_data/R", folder, paste0("/", variable, resolution, "_", folder))
    file_path = ifelse(grepl("nlcd", file_path), 
                        paste0(file_path, "_lulc2011.tif"),
                        paste0(file_path, ".tif")
                        )
    }) ->.;
  .[order(.$variable, .$resolution, .$folder), ] ->.;
}
```


# Download and unzip tiles

To start you need to download the 10-meter NED tiles from the USGS. Because the USGS manages these tiles using an ftp server it's easy for R download them one at a time with the following function. For whatever reason the downloading fuction doesn't work from RStudio, when the Global Option "Use Internet Explorer library/proxy for HTTP" is selected under Packages. Either uncheck or run from the vanilla R console. Beware hiccups with your internet connection may terminate the downloading process.

```{r}
# create list of ned tiles

tiles <- read_sf("D:/geodata/elevation/ned/ned_13arcsec_g.shp", layer = "ned_13arcsec_g")
sapolygon <- read_sf("D:/geodata/soils/SSURGO_R11_FY19.gdb", layer = "SAPOLYGON") %>%
  st_transform("+init=epsg:4326")

idx <- unlist(lapply(st_intersects(tiles, sapolygon), any))
r11 <- tiles[idx, ]
plot(r11[1])

files <- list.files("M:/geodata/elevation/ned/tiles/ArcGrid/13", pattern = ".zip$")


# download 10-meter Region 11

idx <- ! r11$FILE_ID %in% gsub(".zip", "", files)
r11[idx, ] ->.;
split(r112, r112$FILE_ID) ->.;
lapply(., function(x) {
  cat("downloading", x$FILE_ID, as.character(Sys.time()), "\n")
  download_ned_tile("13", x$UL_LAT, abs(x$UL_LON), "M:/geodata/elevation/ned/tiles/ArcGrid")
})


# download 30-meter CONUS

tiles <- read_sf("D:/geodata/elevation/ned/tiles_3_overlap.shp", layer = "tiles_3_overlap")
sapolygon <- read_sf("D:/geodata/soils/SSURGO_CONUS_FY19.gdb", layer = "SAPOLYGON")
# usa <- st_as_sf(maptools::map2SpatialPolygons(maps::map("usa"), IDs = 1:10))
# st_crs(usa) <- "+init=epsg:4326"

idx <- unlist(lapply(st_intersects(tiles, sapolygon), any))
conus <- tiles[idx, ]
plot(conus[1])

files <- list.files("M:/geodata/elevation/ned/tiles/ArcGrid/13", pattern = ".zip$")

idx <- !conus$FILE_ID %in% lf$tiles
conus[idx, ] ->.;
split(., .$FILE_ID) ->.;
lapply(., function(x) {
  cat("getting", x$id, x$FILE_ID, "\n")
  download_ned_tile("1", x$UL_LAT, abs(x$UL_LON), "D:/geodata/elevation/ned/tiles/ArcGrid")
  })


# unzip
path <- "M:/geodata/elevation/ned/tiles/ArcGrid/13"
lf <- data.frame(
  path     = path,
  zipfiles = list.files(path = path, pattern = ".zip$"),
  tiles = NA,
  stringsAsFactors = FALSE
  )
lf <- within(lf, {
  idx         = grepl("USGS", zipfiles)
  tiles[!idx] = gsub(".zip", "", zipfiles[!idx])
  tiles[idx]  = substr(zipfiles[idx], 12, 19)
  
  n           = nchar(tiles)
  tiles       = substr(tiles, n - 6, n)
  
  n           = NULL
  idx         = NULL
  })
lf <- lf[order(lf$id), ]
lf$id <- 1:nrow(tiles)
lf2 <- lf[lf$tiles %in% r11$FILE_ID, ]

split(lf, lf$tiles) ->.;
lapply(., function(x) {
  cat("unzipping", x$id, x$tiles, "\n")
  unzip(zipfile = paste0(x$path, "/", x$zipfiles), 
        exdir   = x$path,
        files   = paste0("grd", x$tiles, "_13/", c("w001001.adf", "w001001x.adf", "dblbnd.adf", "hdr.adf", "prj.adf", "sta.adf"))
        )
  })

# for some reason the following files did not unzip n33w101, n41w76, n25w82
from <- paste0(lf$path, "/grd", lf$tiles, "_1")
to <- paste0(lf$path, "/")
file.copy(from = from, to = to)

```


# Subset NLCD by MLRA office

The NLCD layer is used as a standard coordinate reference system from which to warp the NED mosaics too, and for subseting by MLRA office.

```{r}
nlcd <- "M:/geodata/land_use_land_cover/nlcd_2011_landcover_2011_edition_2014_03_31.img"
sso_dsn <- "M:/geodata/soils/MLRA_Soil_Survey_Areas_Dec2015.shp"
sso_nlcd <-  subset(gd, variable == "nlcd")

subset(gd, folder == "11-JUE" & variable == "nlcd") ->.;
split(., .$file_path) ->.;
lapply(., function(x) {
  crop(nlcd, x$file_path, x$folder, sso_dsn, crsarg)
  })

# Region 11
tiles5 <- read_sf("D:/geodata/elevation/ned/tiles_3_overlapping.shp", layer = "tiles_3_overlapping")

idx <- unlist(lapply(st_intersects(tiles5, st_transform(r11, "+init=epsg:5070")), any))
tiles <- tiles5[idx, ]
plot(tiles[1])

split(tiles, tiles$idx) ->.;
lapply(., function(x) {
  crop("D:/geodata/land_use_land_cover/nlcd_2011_landcover_2011_edition_2014_03_31.img",
       paste0("D:/geodata/land_use_land_cover/nlcd30m_r11_lulc_", x$idx, ".tif"),
       x,
       "+init=epsg:5070"
       )})

```


# Mosaic tile list. 

Beware building the pyramids takes a long time.

```{r}

gd_sub <- subset(gd, variable == "ned" & resolution == "09d" & folder == "11-JUE")
sso_ned_sub <- subset(sso_ned, mlrassoarea == "11-JUE")
idx <- list.files("M:/geodata/elevation/ned/tiles/ArcGrid/13", full.names = TRUE)
idx <- idx[grepl(paste0(sso_ned_sub$FILE_ID, collapse = "|"), idx) & grepl("_13$", idx)]

sso_ned_grid <- file.path(idx, "w001001.adf")

mosaic(sso_ned_grid, gd_sub$file_path, "Float32", c("BIGTIFF=YES"), -99999)


# Region 11
ned_r11 <- list.files(
  path    = "C:/geodata/elevation/ned/tiles/ArcGrid/13",
  pattern = "_13$",
  full.names = TRUE
  )
n <- nchar(ned_r11)
idx <- substr(ned_r11, n - 9, n - 3) %in% r11$FILE_ID
mosaic(paste0(ned_r11[idx], "/w001001.adf"), "C:/geodata/elevation/ned/ned_13as_r11.tif", "Float32", c("BIGTIFF=YES"), -99999)


# CONUS
ned_conus <- list.files(
  path    = "C:/geodata/elevation/ned/tiles/ArcGrid/1",
  pattern = "_1$",
  full.names = TRUE
  )
mosaic(paste0(ned_conus, "/w001001.adf"), "C:/geodata/elevation/ned/ned_1as_conus.tif", "Float32", c("BIGTIFF=YES"), -99999)

```


# Warp NED from a geographic to projected coordinate system 

For warping from EPSG:4326 to EPSG:5070, I've used bilinear resampling which is my personal preference for some of the reasons discussed by Frank Warmerdam (http://courses.neteler.org/gdal-raster-data-tips-and-tricks/). For upscaling or aggregating the 10-meter to 30-meter DEM I use average resampling. Consequentially this makes the most sense and has been the approach used in several studies (Smith et al, 2006; Roecker and Thompson, 2010). Because DEM are later used for terrain analysis they are left uncompressed and untiled, which results in file sizes of approximately 10GB.

```{r}
warp(mo$ned09d.tif, mo$ned10m.tif, mo$nlcd30m.tif, 10, "bilinear", CRSargs(CRS("+init=epsg:4326")), crsarg, "Float32", -99999, c("BIGTIFF=YES"))


# Region 11
# For some reason warping the ned_1as_r11.tif takes forever. Therefore I cut it up prior warping.
lulc <- list.files(
  "D:/geodata/land_use_land_cover",
  recursive = TRUE,
  pattern = "lulc_n..w....tif$"
  )
lulc <- data.frame(
  lulc, 
  tile = unlist(sapply(lulc, function(x) strsplit(x, "_|\\.")[[1]][4])),
  stringsAsFactors = FALSE
  )

# warp
split(tiles, tiles$idx) ->.;
lapply(., function(x) {
  cat("warping", x$tile, as.character(Sys.time()), "\n")
  warp("C:/geodata/elevation/ned/ned_13as_r11.tif",
       paste0("M:/geodata/elevation/ned/ned_10m_r11_", x$tile, ".tif"), 
       paste0("M:/geodata/project_data/11REGION/nlcd30m_11R_lulc_", x$tile, ".tif"), 
       10, "bilinear", 
       CRSargs(CRS("+init=epsg:4326")), 
       CRSargs(CRS("+init=epsg:5070")), 
       "Float32", -99999,
       c("BIGTIFF=YES")
       )
  })

mosaic(paste0("C:/geodata/elevation/ned/ned_10m_r11_", lulc$tile, ".tif"), "C:/geodata/elevation/ned/ned_10m_r11.tif", "Float32", c("BIGTIFF=YES"), -99999)


# crop
split(tiles, tiles$idx) ->.;
lapply(., function(x) {
  crop("D:/geodata/elevation/ned/ned_10m_r11.tif",
       paste0("D:/geodata/elevation/ned/ned_10m_r11_", x$idx, ".tif"),
       x,
       "+init=epsg:5070"
       )})


# resample
split(tiles, tiles$idx) ->.;
lapply(., function(x) {
  resample(paste0("D:/geodata/elevation/ned/ned_10m_r11_", x$idx, ".tif"), 
           paste0("D:/geodata/elevation/ned/ned_30m_r11_", x$idx, ".tif"), 
           30
           )
  })


# CONUS
warp("C:/geodata/elevation/ned/ned_1as_conus.tif", 
     "C:/geodata/elevation/ned/ned_30m_conus.tif", 
     "D:/geodata/land_use_land_cover/nlcd_2011_landcover_2011_edition_2014_03_31.img", 
     30, "bilinear", 
     CRSargs(CRS("+init=epsg:4326")), 
     CRSargs(CRS("+init=epsg:5070")), 
     "Float32", -99999,
     c("BIGTIFF=YES")
     )


vars <- c("folder", "var_res", "file_path")
reshape(gd[vars],
        direction = "wide",
        idvar = "folder",
        timevar = "var_res",
        v.names = "file_path"
        ) ->.;
names(.) <- gsub("file_path.", "", names(.))
# or unstack(gd[vars[2:3]], file_path ~ var_res)
subset(., folder == "11-JUE") ->.;
split(., .$folder) ->.;
lapply(., function(x) {
  warp(x$ned09d, x$ned10m, x$nlcd30m, 10, "bilinear", CRSargs(CRS("+init=epsg:4326")), crsarg, "Float32", -99999, c("BIGTIFF=YES"))
})


resample(mo$ned10m.tif, 30)
```


# Calculate hillshade, slope, and aspect

GDALs DEM tools use Horn'n (1981) algorithms as the default, as does ArcInfo and GRASS.

```{r}

subset(gd, folder == "11-JUE" & var_res == "ned10m") ->.;
split(., .$folder) ->.;
lapply(., function(x) {
  dem(x$file_path, c("TILED=YES", "COMPRESS=DEFLATE", "BIGTIFF=YES"))
  })


# Region 11
dem("C:/geodata/elevation/ned/ned_10m_r11.tif", c("TILED=YES", "COMPRESS=DEFLATE", "BIGTIFF=YES"))

# CONUS
dem("C:/geodata/elevation/ned/ned_30m_conus.tif", c("TILED=YES", "COMPRESS=DEFLATE", "BIGTIFF=YES"))

```


# Mosaic the 30-meter MLRA office mosaics into a mlraoffice office mosaic

```{r}
batch_mosaic(list(mo$ned30m.tif), ro$ned30m.tif)
batch_mosaic(list(mo$hil10m.tif), ro$hil10m.tif, "Byte", c("COMPRESS=DEFLATE", "TILED=YES", "BIGTIFF=YES"), 0)
batch_mosaic(list(mo$slp10m.tif), ro$slp10m.tif, "Byte", c("COMPRESS=DEFLATE", "TILED=YES", "BIGTIFF=YES"), 0)
batch_mosaic(list(mo$nlcd30m.tif), ro$nlc30m.tif, "Byte", c("COMPRESS = DEFLATE", "TILED = YES", "BIGTIFF = YES"), 0)
```
